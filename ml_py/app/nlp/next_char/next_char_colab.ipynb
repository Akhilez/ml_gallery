{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "next_char.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KSCudkBdP1h",
        "outputId": "3fb758db-6425-4935-a1bd-546da15d8103"
      },
      "source": [
        "!pip install torchtext==0.8.0\n",
        "!pip freeze | grep torchtext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.8.0\n",
            "torchtext==0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQjqIvdXYRBw",
        "outputId": "835fe7d4-85bd-400b-df03-3f9f575f9e2d"
      },
      "source": [
        "!rm -rf *\n",
        "!git config --global user.name \"Akhilez\"\n",
        "!git config --global user.email \"akhild18@yahoo.com\"\n",
        "!git clone https://github.com/Akhilez/ml_gallery.git\n",
        "%cd ml_gallery/ml_py"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ml_gallery'...\n",
            "remote: Enumerating objects: 787, done.\u001b[K\n",
            "remote: Counting objects: 100% (787/787), done.\u001b[K\n",
            "remote: Compressing objects: 100% (516/516), done.\u001b[K\n",
            "remote: Total 3962 (delta 471), reused 553 (delta 256), pack-reused 3175\u001b[K\n",
            "Receiving objects: 100% (3962/3962), 41.09 MiB | 40.81 MiB/s, done.\n",
            "Resolving deltas: 100% (2428/2428), done.\n",
            "/content/ml_gallery/ml_py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJhgOYRWdc-K",
        "outputId": "8f430692-9b18-427c-c69c-05ad0f1f7aee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "models_path = '/content/gdrive/MyDrive/Projects/ML/next_char'"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGRSsYXmYcgH",
        "outputId": "360af155-0a04-4fd4-f171-382b4150c64a"
      },
      "source": [
        "import os\n",
        "os.environ['SECRET_KEY'] = '1234'\n",
        "from mlg.settings import BASE_DIR\n",
        "os.environ['BASE'] = BASE_DIR\n",
        "%mkdir -p ${BASE}/data/subtitles\n",
        "%mkdir -p ${BASE}/models\n",
        "!wget -O ${BASE}/data/subtitles/cleaned.txt https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned.txt\n",
        "!wget -O ${BASE}/data/subtitles/cleaned_test.txt https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned_test.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-06 05:01:40--  https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.15.80, 172.253.122.128, 172.217.7.144, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.15.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 349823 (342K) [text/plain]\n",
            "Saving to: ‘/content/ml_gallery/ml_py/data/subtitles/cleaned.txt’\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/ml_gallery 100%[===================>] 341.62K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2020-12-06 05:01:40 (152 MB/s) - ‘/content/ml_gallery/ml_py/data/subtitles/cleaned.txt’ saved [349823/349823]\n",
            "\n",
            "--2020-12-06 05:01:40--  https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned_test.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.5.240, 172.217.15.80, 172.253.63.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.5.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6726 (6.6K) [text/plain]\n",
            "Saving to: ‘/content/ml_gallery/ml_py/data/subtitles/cleaned_test.txt’\n",
            "\n",
            "/content/ml_gallery 100%[===================>]   6.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-06 05:01:40 (72.7 MB/s) - ‘/content/ml_gallery/ml_py/data/subtitles/cleaned_test.txt’ saved [6726/6726]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljr_oefiZBCO"
      },
      "source": [
        "import torch\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "from mlg.settings import BASE_DIR\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from datetime import datetime"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihTYv6KvZmJ9",
        "outputId": "92019ad1-9120-406c-add0-ce8239fcd5a6"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "cleaned_data_path = f'{BASE_DIR}/data/subtitles/cleaned_test.txt'\n",
        "data_path = f'{BASE_DIR}/data/subtitles'\n",
        "\n",
        "batch_size = 64\n",
        "seq_len = 25\n",
        "\n",
        "pad_tkn = '~'\n",
        "unk_tkn = '*'\n",
        "eos_tkn = '\\n'\n",
        "init_tkn = '>'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-He4aGnvaFXL",
        "outputId": "29acf14e-31a6-4819-f1a9-8270d554d826"
      },
      "source": [
        "TEXT = Field(sequential=True, tokenize=list, fix_length=seq_len, unk_token=unk_tkn, pad_first=False,\n",
        "             pad_token=pad_tkn, eos_token=eos_tkn, init_token=init_tkn)\n",
        "\n",
        "train_dataset, test_dataset = TabularDataset.splits(\n",
        "    path=data_path,\n",
        "    train='cleaned.txt', test='cleaned_test.txt',\n",
        "    format='csv',\n",
        "    skip_header=False,\n",
        "    fields=[(\"text\", TEXT)])\n",
        "\n",
        "TEXT.build_vocab(train_dataset)\n",
        "vocab_size = len(TEXT.vocab.itos)\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(\n",
        "    (train_dataset, test_dataset),\n",
        "    batch_sizes=(batch_size, batch_size),\n",
        "    device=device,\n",
        "    sort_key=lambda txt: len(txt.text),\n",
        "    sort_within_batch=False,\n",
        "    repeat=True\n",
        ")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHp3KocVa5z7"
      },
      "source": [
        "class NextCharModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embed = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self.embed_size\n",
        "        )\n",
        "\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=self.embed_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            nonlinearity='relu'\n",
        "        )\n",
        "\n",
        "        self.y = nn.Linear(self.hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.embed(x))\n",
        "        y, _ = self.rnn(y)\n",
        "        return F.softmax(self.y(y), 2)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOIwZNbsgeDF"
      },
      "source": [
        "def load_model(latest=True, name=None):\n",
        "    if latest:\n",
        "        model_name = max(os.listdir(models_path))\n",
        "        model = NextCharModel(vocab_size, 512, 512)\n",
        "        model.load_state_dict(torch.load(f'{models_path}/{model_name}')).to(device)\n",
        "        return model"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBRUy74AlB-n"
      },
      "source": [
        "model = NextCharModel(vocab_size, 32, 128).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFc5GGrTa8fl",
        "outputId": "d13ee63b-c6b6-46a8-f26b-2f6b95d954f4"
      },
      "source": [
        "epochs = 5\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    i = 0\n",
        "    losses = 0\n",
        "    print(f'Epoch: {epoch}')\n",
        "    for batch in train_iter:\n",
        "        x_batch = batch.text\n",
        "        y_batch = x_batch[1:]\n",
        "        x_batch = x_batch[:-1]\n",
        "\n",
        "        y_pred = model(x_batch)\n",
        "        loss = loss_fn(y_pred.view((-1, vocab_size)), y_batch.flatten())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses += loss.item()\n",
        "\n",
        "        i+=1\n",
        "        if i % 3000 == 0:\n",
        "            print(i / 3000, losses / 3000)\n",
        "            losses = 0\n",
        "\n",
        "        if i % 30000 == 0:\n",
        "            test_sentence = \"Hey, wha\"\n",
        "            pred = predict(test_sentence)\n",
        "            print(f'\"{pred}\"')\n",
        "            model.train()\n",
        "\n",
        "        if i % 300000 == 0:\n",
        "            save_model(model, f'epoch{epoch}_batch{i}')\n",
        "\n",
        "    save_model(model) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.0 3.5008288300832113\n",
            "2.0 3.4522635898590086\n",
            "3.0 3.4447192271550495\n",
            "4.0 3.435407151858012\n",
            "5.0 3.415821606953939\n",
            "6.0 3.4096887852350872\n",
            "7.0 3.40635427292188\n",
            "8.0 3.405636769294739\n",
            "9.0 3.4036244746049245\n",
            "10.0 3.4031432440280915\n",
            "\"Hey, wha the te the te the te the te the te the te\"\n",
            "11.0 3.402107408285141\n",
            "12.0 3.401642345905304\n",
            "13.0 3.400917260567347\n",
            "14.0 3.400035128513972\n",
            "15.0 3.397051036755244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wncoclx5a_OZ",
        "outputId": "3e6ad177-07a3-48a7-ed88-2cff1f7aeaf5"
      },
      "source": [
        "\n",
        "def predict(sentence):\n",
        "    terminal_chars = [eos_tkn, '\\n', pad_tkn]\n",
        "    max_len = 50\n",
        "    next_char = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        while next_char not in terminal_chars and len(sentence) < max_len:\n",
        "            seq = torch.tensor([TEXT.vocab[s] or TEXT.vocab[unk_tkn] for s in list(sentence.lower())], device=device, dtype=torch.long).view((-1, 1))\n",
        "            preds = model(seq)\n",
        "            m = int(preds[-1][0].argmax())\n",
        "            next_char = TEXT.vocab.itos[m]\n",
        "            sentence = sentence + next_char\n",
        "    return sentence\n",
        "\n",
        "\n",
        "test_sentence = \"Hey, what's u\"\n",
        "\n",
        "pred = predict(test_sentence)\n",
        "print(f'\"{pred}\"')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Hey, what's u~\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVbYHnveds1w"
      },
      "source": [
        "last_saved_model_path = ''\n",
        "def save_model(model, message):\n",
        "    last_saved_model_path = f'{models_path}/model_{int(datetime.now().timestamp())}{f\"_{message}\" if message else \"\"}.pt'\n",
        "    torch.save(model.state_dict, last_saved_model_path)\n",
        "save_model(model, message='test')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgx5gi6Ab2gX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}