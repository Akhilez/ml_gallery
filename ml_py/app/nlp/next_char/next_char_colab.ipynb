{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "next_char.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4KSCudkBdP1h",
    "outputId": "11a73d31-cd18-470b-d5e5-8e0c1dc8edc6"
   },
   "source": [
    "!pip install torchtext==0.8.0\n",
    "!pip freeze | grep torchtext"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.8.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/23/23/8499af6d9c22b29b01f66a2c11d38ce71cd1cafa2655913c29818ed4a00f/torchtext-0.8.0-cp36-cp36m-manylinux1_x86_64.whl (6.9MB)\n",
      "\u001B[K     |████████████████████████████████| 6.9MB 19.2MB/s \n",
      "\u001B[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.7.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (1.18.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.8.0) (0.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (2020.11.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
      "Installing collected packages: torchtext\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "Successfully installed torchtext-0.8.0\n",
      "torchtext==0.8.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQjqIvdXYRBw",
    "outputId": "bbcd9da7-abb7-40cc-aa3f-7a27454eb1bf"
   },
   "source": [
    "!rm -rf ml_gallery\n",
    "!git config --global user.name \"Akhilez\"\n",
    "!git config --global user.email \"akhild18@yahoo.com\"\n",
    "!git clone https://github.com/Akhilez/ml_gallery.git\n",
    "%cd ml_gallery/ml_py"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'ml_gallery'...\n",
      "remote: Enumerating objects: 843, done.\u001B[K\n",
      "remote: Counting objects: 100% (843/843), done.\u001B[K\n",
      "remote: Compressing objects: 100% (549/549), done.\u001B[K\n",
      "remote: Total 4018 (delta 502), reused 595 (delta 273), pack-reused 3175\u001B[K\n",
      "Receiving objects: 100% (4018/4018), 41.12 MiB | 41.40 MiB/s, done.\n",
      "Resolving deltas: 100% (2459/2459), done.\n",
      "/content/ml_gallery/ml_py\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJhgOYRWdc-K",
    "outputId": "7b544f7e-fff9-49f8-87de-ffef7449ebf0"
   },
   "source": [
    "# Skip if you don't want to connect to gdrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QtJBD8TXN6n2"
   },
   "source": [
    "models_path = '/content/gdrive/MyDrive/Projects/ML/next_char'\n",
    "!mkdir -p /content/gdrive/MyDrive/Projects/ML/next_char"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGRSsYXmYcgH",
    "outputId": "49757fb7-078c-45d9-982e-02f2058bd559"
   },
   "source": [
    "import os\n",
    "os.environ['SECRET_KEY'] = '1234'\n",
    "from mlg.settings import BASE_DIR\n",
    "os.environ['BASE'] = BASE_DIR\n",
    "%mkdir -p ${BASE}/data/subtitles\n",
    "%mkdir -p ${BASE}/models\n",
    "!wget -O ${BASE}/data/subtitles/cleaned.txt https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned.txt\n",
    "!wget -O ${BASE}/data/subtitles/cleaned_test.txt https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned_test.txt"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2020-12-07 17:20:54--  https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.5.240, 172.217.13.80, 172.217.15.80, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.5.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 349823 (342K) [text/plain]\n",
      "Saving to: ‘/content/ml_gallery/ml_py/data/subtitles/cleaned.txt’\n",
      "\n",
      "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/ml_gallery 100%[===================>] 341.62K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2020-12-07 17:20:54 (95.9 MB/s) - ‘/content/ml_gallery/ml_py/data/subtitles/cleaned.txt’ saved [349823/349823]\n",
      "\n",
      "--2020-12-07 17:20:54--  https://storage.googleapis.com/akhilez/datasets/marvel_subtitles/cleaned_test.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.5.240, 172.217.7.240, 172.217.13.80, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.5.240|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6726 (6.6K) [text/plain]\n",
      "Saving to: ‘/content/ml_gallery/ml_py/data/subtitles/cleaned_test.txt’\n",
      "\n",
      "/content/ml_gallery 100%[===================>]   6.57K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-07 17:20:55 (54.0 MB/s) - ‘/content/ml_gallery/ml_py/data/subtitles/cleaned_test.txt’ saved [6726/6726]\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ljr_oefiZBCO"
   },
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from mlg.settings import BASE_DIR\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihTYv6KvZmJ9",
    "outputId": "07be483a-784e-47e3-f422-04529d6343da"
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "cleaned_data_path = f'{BASE_DIR}/data/subtitles/cleaned_test.txt'\n",
    "data_path = f'{BASE_DIR}/data/subtitles'\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 64\n",
    "\n",
    "pad_tkn = '~'\n",
    "unk_tkn = '*'\n",
    "eos_tkn = '\\n'\n",
    "init_tkn = '>'"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "cuda\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-He4aGnvaFXL",
    "outputId": "048a6e4b-5f56-4911-f558-f7594099d0ab"
   },
   "source": [
    "TEXT = Field(sequential=True, tokenize=list, fix_length=seq_len, unk_token=unk_tkn, pad_first=False,\n",
    "             pad_token=pad_tkn, eos_token=eos_tkn, init_token=init_tkn)\n",
    "\n",
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "    path=data_path,\n",
    "    train='cleaned.txt', test='cleaned_test.txt',\n",
    "    format='csv',\n",
    "    skip_header=False,\n",
    "    fields=[(\"text\", TEXT)])\n",
    "\n",
    "TEXT.build_vocab(train_dataset)\n",
    "vocab_size = len(TEXT.vocab.itos)\n",
    "# torch.save(TEXT.vocab, f'{models_path}/vocab.pt')\n",
    "\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train_dataset, test_dataset),\n",
    "    batch_sizes=(batch_size, batch_size),\n",
    "    device=device,\n",
    "    sort_key=lambda txt: len(txt.text),\n",
    "    sort_within_batch=False,\n",
    "    repeat=True\n",
    ")\n"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LHp3KocVa5z7"
   },
   "source": [
    "class NextCharModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=self.embed_size\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embed_size,\n",
    "            hidden_size=self.hidden_size\n",
    "            #nonlinearity='relu'\n",
    "        )\n",
    "\n",
    "        self.y = nn.Linear(self.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.embed(x))\n",
    "        y, _ = self.rnn(y)\n",
    "        return F.softmax(self.y(y), 2)\n"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TOIwZNbsgeDF"
   },
   "source": [
    "def load_model(latest=True, name=None):\n",
    "    if latest:\n",
    "        model_name = max(os.listdir(models_path))\n",
    "        model = NextCharModel(vocab_size, 512, 1024)\n",
    "        print(f\"Loading model {model_name}\")\n",
    "        model.load_state_dict(torch.load(f'{models_path}/{model_name}'))\n",
    "        return model.to(device)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IVbYHnveds1w"
   },
   "source": [
    "last_saved_model_path = ''\n",
    "def save_model(model, message=None):\n",
    "    last_saved_model_path = f'{models_path}/model_{int(datetime.now().timestamp())}{f\"_{message}\" if message else \"\"}.pt'\n",
    "    torch.save(model.state_dict(), last_saved_model_path)\n",
    "#save_model(model, message='test')"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OBRUy74AlB-n"
   },
   "source": [
    "model = NextCharModel(vocab_size, 512, 1024).to(device)\n",
    "# model = load_model()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wncoclx5a_OZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ee63939d-d5fa-4950-fef9-1c7564864456"
   },
   "source": [
    "def predict(sentence):\n",
    "    terminal_chars = [eos_tkn, '\\n', pad_tkn]\n",
    "    max_len = 50\n",
    "    next_char = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while next_char not in terminal_chars and len(sentence) < max_len:\n",
    "            seq = torch.tensor([TEXT.vocab[s] or TEXT.vocab[unk_tkn] for s in list(sentence.lower())], device=device, dtype=torch.long).view((-1, 1))\n",
    "            preds = model(seq)\n",
    "            m = int(preds[-1][0].argmax())\n",
    "            next_char = TEXT.vocab.itos[m]\n",
    "            sentence = sentence + next_char\n",
    "    return sentence\n",
    "\n",
    "\n",
    "test_sentence = \"Hey, what's u\"\n",
    "\n",
    "pred = predict(test_sentence)\n",
    "print(f'\"{pred}\"')\n"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\"Hey, what's u8888pppp\n",
      "\"\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "OFc5GGrTa8fl",
    "outputId": "3d2ea23d-4398-4077-cce7-ae7195a3549c"
   },
   "source": [
    "model.train()\n",
    "log_freq = 500\n",
    "i = 0\n",
    "losses = 0\n",
    "for batch in train_iter:\n",
    "    x_batch = batch.text\n",
    "    y_batch = x_batch[1:]\n",
    "    x_batch = x_batch[:-1]\n",
    "\n",
    "    y_pred = model(x_batch)\n",
    "    loss = loss_fn(y_pred.view((-1, vocab_size)), y_batch.flatten())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses += loss.item()\n",
    "\n",
    "    i+=1\n",
    "    if i % log_freq == 0:\n",
    "        print(i / log_freq, losses / log_freq)\n",
    "        losses = 0\n",
    "\n",
    "    if i % (log_freq * 10) == 0:\n",
    "        test_sentence = \"Hey, wha\"\n",
    "        pred = predict(test_sentence)\n",
    "        print(f'\"{pred}\"')\n",
    "        model.train()\n",
    "\n",
    "    if i % (log_freq * 50) == 0:\n",
    "        print(\"Saving model\")\n",
    "        save_model(model, f'batch{i}')\n",
    "\n",
    "save_model(model) "
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "1.0 3.1961354751586915\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-22-1d1f39cce3d0>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    130\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 132\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    133\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pgx5gi6Ab2gX"
   },
   "source": [
    "save_model(model)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}