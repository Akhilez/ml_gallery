{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from app.rl.alpha_nine.a9 import A9Model, convert_inputs\n",
    "from gym_nine_mens_morris.envs.nine_mens_morris_env import NineMensMorrisEnv, Pix\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "model = A9Model(8, 1, 1, 1).double().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-19c76bbef253>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    211\u001B[0m             \u001B[0mplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mAIPlayer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrandom_player\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrender\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    212\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 213\u001B[0;31m \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    214\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-19c76bbef253>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    154\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m                         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopponent\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menvs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlegal_actions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 156\u001B[0;31m                         \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_done\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menvs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m                     \u001B[0mis_dones\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mis_done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/gyms/gym-nine-mens-morris/gym_nine_mens_morris/envs/nine_mens_morris_env.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, position, move, kill_location)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0mmoved_position\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_moved_position\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mposition\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmove\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[0mis_phase_1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0munused\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m         \u001B[0mis_illegal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_action_illegal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mposition\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmoved_position\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_phase_1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkill_location\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    131\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_illegal\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mboard\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_done\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m'code'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mis_illegal\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/gyms/gym-nine-mens-morris/gym_nine_mens_morris/envs/nine_mens_morris_env.py\u001B[0m in \u001B[0;36m_is_action_illegal\u001B[0;34m(self, position, moved_position, is_phase_1, kill_location)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mis_phase_1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 275\u001B[0;31m             \u001B[0;32mif\u001B[0m \u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mboard\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mposition\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mPix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mS\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    276\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mInfoCode\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbad_action_position\u001B[0m  \u001B[0;31m# \"During phase 1, the position must be empty.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    277\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Phase 2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "\n",
    "def sample_action(legal_actions, pos1, pos2, move, kill, is_phase_1, argmax=False):\n",
    "\n",
    "    probs = []\n",
    "\n",
    "    legal_pos = list(set([action[0] for action in legal_actions]))  # [(3, 2, 4), (3, 2, 4), ... ]\n",
    "    pos_probs = pos1 if is_phase_1 else pos2\n",
    "    pos_idx = int(pos_probs.argmax()) if argmax else int(torch.multinomial(pos_probs, 1).squeeze())  # 24\n",
    "    pos = legal_pos[pos_idx]  # (3, 2, 4)\n",
    "    probs.append(pos_probs[pos_idx])\n",
    "\n",
    "    # [0, 1, 2, 3]\n",
    "    legal_moves = list(set([action[1] for action in legal_actions if tuple(action[0]) == tuple(pos) and action[1] is not None]))\n",
    "    if len(legal_moves) != 0:\n",
    "        move_idx = int(move.argmax()) if argmax else int(torch.multinomial(move, 1).squeeze())  # 4\n",
    "        move_ = legal_moves[move_idx]  # 4\n",
    "        probs.append(move[move_idx])\n",
    "    else:\n",
    "        move_ = None\n",
    "\n",
    "    legal_kills = list(set([action[2] for action in legal_actions if tuple(action[0]) == tuple(pos) and action[2] is not None]))\n",
    "    if len(legal_kills) != 0:\n",
    "        kill_idx = int(kill.argmax()) if argmax else int(torch.multinomial(kill, 1).squeeze())\n",
    "        kill_ = legal_kills[kill_idx]  # (3, 2, 4)\n",
    "        probs.append(kill[kill_idx])\n",
    "    else:\n",
    "        kill_ = None\n",
    "\n",
    "    return pos, move_, kill_, torch.mean(torch.stack(probs))\n",
    "\n",
    "\n",
    "def play(player_1, player_2, render=False):\n",
    "    env = NineMensMorrisEnv()\n",
    "    env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    info = {}\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        player = player_1 if env.player == Pix.W else player_2\n",
    "        action_pos, move, kill_pos = player(env)\n",
    "        state, reward, is_done, info = env.step(action_pos, move, kill_pos)\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    won = info.get('won')\n",
    "    if won:\n",
    "        return 1 if won == Pix.W.string else 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def random_player(env, legal_actions=None):\n",
    "    legal_actions = legal_actions if legal_actions is not None else env.get_legal_actions()\n",
    "    random_idx = int(torch.randint(low=0, high=len(legal_actions), size=(1,))[0])\n",
    "    return legal_actions[random_idx]\n",
    "\n",
    "\n",
    "class AIPlayer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, env, legal_actions=None):\n",
    "        legal_actions = legal_actions if legal_actions is not None else env.get_legal_actions()\n",
    "        xs = [convert_inputs(env.board, env.player)]\n",
    "        xs = torch.stack(xs).long().to(device)\n",
    "        was_train = self.model.training\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            yh_pos_1, yh_pos_2, yh_move, yh_kill = self.model(xs)  # yh shape: (batch, 9)\n",
    "\n",
    "        if was_train:\n",
    "            self.model.train()\n",
    "\n",
    "        pos, move, kill, _ = sample_action(legal_actions, yh_pos_1[0], yh_pos_2[0], yh_move[0], yh_kill[0], env.is_phase_1(), argmax=True)\n",
    "\n",
    "        return pos, move, kill\n",
    "\n",
    "\n",
    "def get_credits(t, gamma):\n",
    "    credits = []\n",
    "    prev_credit = 1\n",
    "    for i in range(t):\n",
    "        credits.append(prev_credit)\n",
    "        prev_credit *= gamma\n",
    "    return torch.tensor(list(reversed(credits))).double().to(device)\n",
    "\n",
    "\n",
    "def get_returns(stats, gamma):\n",
    "    total_t = len(stats)\n",
    "    returns = []\n",
    "    prev_return = 0\n",
    "    for t in range(total_t):\n",
    "        prev_return = stats[total_t - t -1][1] + (gamma * prev_return)\n",
    "        returns.append(prev_return)\n",
    "    return torch.tensor(list(reversed(returns))).double().to(device)\n",
    "\n",
    "\n",
    "def get_loss(stats):\n",
    "    loss = 0\n",
    "    for i_env in range(len(stats)):\n",
    "        returns = get_returns(stats[i_env], gamma=0.75)\n",
    "\n",
    "        probs = torch.stack([stat[0] for stat in stats[i_env]])\n",
    "        probs = torch.log(probs)\n",
    "\n",
    "        credits = get_credits(len(stats[i_env]), gamma=0.75)\n",
    "\n",
    "        loss += torch.sum(probs * credits * returns) / len(stats[i_env])\n",
    "    return -1 * loss / len(stats)\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 10000\n",
    "\n",
    "    model.train()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    envs = [NineMensMorrisEnv() for _ in range(batch_size)]\n",
    "    losses = []\n",
    "    opponent = AIPlayer(copy.deepcopy(model))\n",
    "    prev_models = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Before game starts\n",
    "\n",
    "        me_idx_bool = torch.arange(batch_size) > (batch_size - 1) / 2\n",
    "        stats = [[] for _ in range(batch_size)]\n",
    "        is_dones = [False for _ in range(batch_size)]\n",
    "        _ = [env.reset() for env in envs]\n",
    "        ai_pieces = [Pix.W if me_idx_bool[i] else Pix.B for i in range(batch_size)]\n",
    "\n",
    "        # Monte Carlo loop\n",
    "        for t in range(15):\n",
    "            legal_actions = [env.get_legal_actions() for env in envs]\n",
    "\n",
    "            xs = [convert_inputs(envs[i].board, envs[i].player) for i in range(batch_size)]\n",
    "            xs = torch.stack(xs).long().to(device)\n",
    "\n",
    "            yh_pos_1, yh_pos_2, yh_move, yh_kill = model(xs)  # yh shape: (batch, 9)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                if not is_dones[i]:\n",
    "\n",
    "                    # Is current player AI or other?\n",
    "                    if envs[i].player == ai_pieces[i]:\n",
    "\n",
    "                        pos, move, kill, prob = sample_action(legal_actions, yh_pos_1[i], yh_pos_2[i], yh_move[i], yh_kill[i], envs[i].is_phase_1())\n",
    "\n",
    "                        state, reward, is_done, info = envs[i].step(pos, move, kill)\n",
    "\n",
    "                        stats[i].append([prob, reward])\n",
    "                    else:\n",
    "                        action = opponent(envs[i], legal_actions[i])\n",
    "                        _, _, is_done, _ = envs[i].step(action)\n",
    "\n",
    "                    is_dones[i] = is_done\n",
    "\n",
    "            if all(is_dones):\n",
    "                break\n",
    "\n",
    "        loss = get_loss(stats)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            plays = [play(AIPlayer(model), opponent) for _ in range(50)]\n",
    "            plays = plays + [play(opponent, AIPlayer(model)) for _ in range(50)]\n",
    "            winners = torch.tensor(plays).float()\n",
    "\n",
    "            draws = len(torch.nonzero(winners == 0))\n",
    "            wins = len(torch.nonzero(winners == 1))\n",
    "            loses = len(torch.nonzero(winners == 2))\n",
    "\n",
    "            print(f'{epoch}: {np.mean(losses)}, plays: {draws, wins, loses}')\n",
    "\n",
    "            # if (epoch + 1) % 1000 == 0:\n",
    "            # plt.plot(losses)\n",
    "            # plt.show()\n",
    "\n",
    "            losses = []\n",
    "            prev_models = prev_models[-10:]\n",
    "            prev_models.append(copy.deepcopy(model))\n",
    "            opponent.model = prev_models[np.random.choice(len(prev_models), 1)[0]]\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            plays = [play(AIPlayer(model), random_player) for _ in range(100)]\n",
    "            winners = torch.tensor(plays).float()\n",
    "\n",
    "            draws = len(torch.nonzero(winners == 0))\n",
    "            wins = len(torch.nonzero(winners == 1))\n",
    "            loses = len(torch.nonzero(winners == 2))\n",
    "            print(f\"Against random guy: \", end='')\n",
    "            print(draws, wins, loses, end=', ')\n",
    "\n",
    "            plays = [play(random_player, AIPlayer(model)) for _ in range(100)]\n",
    "            winners = torch.tensor(plays).float()\n",
    "\n",
    "            draws = len(torch.nonzero(winners == 0))\n",
    "            wins = len(torch.nonzero(winners == 1))\n",
    "            loses = len(torch.nonzero(winners == 2))\n",
    "\n",
    "            print(draws, wins, loses)\n",
    "\n",
    "            play(random_player, AIPlayer(model), render=True)\n",
    "            play(AIPlayer(model), random_player, render=True)\n",
    "\n",
    "train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plays = [play(AIPlayer(model), random_player) for _ in range(100)]\n",
    "winners = torch.tensor(plays).float()\n",
    "\n",
    "draws = len(torch.nonzero(winners == 0))\n",
    "wins = len(torch.nonzero(winners == 1))\n",
    "loses = len(torch.nonzero(winners == 2))\n",
    "\n",
    "draws, wins, loses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plays = [play(random_player, AIPlayer(model)) for _ in range(100)]\n",
    "winners = torch.tensor(plays).float()\n",
    "\n",
    "draws = len(torch.nonzero(winners == 0))\n",
    "wins = len(torch.nonzero(winners == 1))\n",
    "loses = len(torch.nonzero(winners == 2))\n",
    "\n",
    "draws, wins, loses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(random_player, AIPlayer(model), render=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}