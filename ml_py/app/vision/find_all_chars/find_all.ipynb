{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import ops\n",
    "\n",
    "from lib import detection_utils as utils\n",
    "from lib.mnist_aug.mnist_augmenter import DataManager, MNISTAug\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "k = 9\n",
    "H = 112\n",
    "W = 112\n",
    "Wp = 22\n",
    "Hp = 22\n",
    "b_regions = 256\n",
    "\n",
    "threshold_p = 0.6\n",
    "threshold_n = 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dm = DataManager()\n",
    "dm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "aug = MNISTAug()\n",
    "x_train, y_train = aug.get_augmented(dm.x_train, dm.y_train, 10)\n",
    "x_test, y_test = aug.get_augmented(dm.x_test, dm.y_test, 2)\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).view((-1, 1, H, W))\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).view((-1, 1, H, W))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class MnistDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.threshold_p = 0.6\n",
    "        self.threshold_n = 0.3\n",
    "\n",
    "        self.Wp = 22\n",
    "        self.Hp = 22\n",
    "\n",
    "        self.X = 28  # Width of region\n",
    "        self.Y = 28\n",
    "\n",
    "        self.b_regions = 256\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        self.DetectorOut = collections.namedtuple('DetectorOut', 'features confidences diffs regions_p regions_n idx_p idx_n matched_bboxes')\n",
    "        self.anchors_tensor = utils.generate_anchors(shape=(Wp, Hp), sizes=(.15, .45, .75),\n",
    "                                        ratios=(0.5, 1, 2))  # Tensor of shape (4, k*H*W) -> cy, cy, w, h\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        self.box_regressor = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 5 * self.k, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y_bboxes=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        x: tensor of shape (-1, C, H, W)\n",
    "        bboxes: (optional) list of tensors of shape (4, n)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(x)\n",
    "        bboxes = self.box_regressor(features)\n",
    "        bboxes = bboxes.view((-1, 5, k, *bboxes.shape[-2:]))\n",
    "\n",
    "        regions_p = []\n",
    "        regions_n = []\n",
    "        idx_p_batch = []\n",
    "        idx_n_batch = []\n",
    "        best_bbox_idx_batch = []\n",
    "\n",
    "        # If training mode, then sample positives and negatives, extract regions\n",
    "        if self.training and y_bboxes is not None:\n",
    "            for i_batch in range(len(x)):\n",
    "                iou = utils.get_iou_map(y_bboxes[i], self.anchors_tensor)\n",
    "                iou = utils.raise_bbox_iou(iou, self.threshold_p)\n",
    "                iou_max, iou_argmax = torch.max(iou, 0)  # Shape (k*H*W)\n",
    "\n",
    "                # Random sampling\n",
    "                idx_p, idx_n = utils.sample_pn_indices(iou_max, self.threshold_p, self.threshold_n, self.b_regions)\n",
    "\n",
    "                # Get off-set boxes\n",
    "                pred_bbox_p, pred_bbox_n = utils.get_pred_boxes(bboxes[i, 1:], self.anchors_tensor, idx_p, idx_n)  # (4, n) (cx, cy, w, h)\n",
    "\n",
    "                # Remove tiny boxes\n",
    "                big_box_indices_p = utils.get_tiny_box_indices(pred_bbox_p, 0.05)\n",
    "                big_box_indices_n = utils.get_tiny_box_indices(pred_bbox_n, 0.05)\n",
    "                pred_bbox_p = pred_bbox_p[:, big_box_indices_p]\n",
    "                pred_bbox_n = pred_bbox_n[:, big_box_indices_n]\n",
    "                idx_p = idx_p[big_box_indices_p]\n",
    "                idx_n = idx_n[big_box_indices_n]\n",
    "\n",
    "                # Change format from (cx cy w h) to (x1 y1 x2 y2)\n",
    "                pred_bbox_p = utils.centers_to_diag(pred_bbox_p)  # shape (4, p) (x1y1x2y2)\n",
    "                pred_bbox_n = utils.centers_to_diag(pred_bbox_n)\n",
    "\n",
    "                # Make record of these\n",
    "                idx_p_batch.append(idx_p)\n",
    "                idx_n_batch.append(idx_n)\n",
    "                best_bbox_idx_batch.append(iou_argmax)\n",
    "\n",
    "                # De-Normalize - Make coordinates feature indices b/w H and W\n",
    "                multiplier = torch.tensor([self.Wp, self.Hp, self.Wp, self.Hp]).view((4, 1))\n",
    "                pred_bbox_p = (pred_bbox_p * multiplier).round().type(torch.int32)  # shape (4, p) (x1y1x2y2)\n",
    "                pred_bbox_n = (pred_bbox_n * multiplier).round().type(torch.int32)\n",
    "\n",
    "                # Clip boxes that are out of range\n",
    "                pred_bbox_p = ops.clip_boxes_to_image(pred_bbox_p.T, (self.Hp, self.Wp)).T\n",
    "                pred_bbox_n = ops.clip_boxes_to_image(pred_bbox_n.T, (self.Hp, self.Wp)).T\n",
    "\n",
    "                # Make crops of features\n",
    "                regions_batch = []\n",
    "                for positive_idx in range(len(idx_p)):\n",
    "                    idx = pred_bbox_p[:, positive_idx]\n",
    "                    cropped = features[i_batch, :, idx[0]:idx[2]+1, idx[1]:idx[3]+1]\n",
    "                    cropped = F.interpolate(cropped.view((1, *cropped.shape)), (self.X, self.Y), mode='bilinear')[0]\n",
    "                    regions_batch.append(cropped)\n",
    "                regions_batch = torch.stack(regions_batch)\n",
    "                regions_p.append(regions_batch)\n",
    "\n",
    "                regions_batch = []\n",
    "                for negative_idx in range(len(idx_n)):\n",
    "                    idx = pred_bbox_n[:, negative_idx]\n",
    "                    cropped = features[i_batch, :, idx[0]:idx[2]+1, idx[1]:idx[3]+1]\n",
    "                    cropped = F.interpolate(cropped.view((1, *cropped.shape)), (self.X, self.Y), mode='bilinear')[0]\n",
    "                    regions_batch.append(cropped)\n",
    "                regions_n.append(torch.stack(regions_batch))\n",
    "\n",
    "        # TODO: If eval mode, then sample top 300 confidence anchors' regions\n",
    "        if not self.training:\n",
    "            pass\n",
    "\n",
    "        return self.DetectorOut(\n",
    "            features=features,\n",
    "            confidences=bboxes[:, 0] if bboxes is not None else None,\n",
    "            diffs=bboxes[:, 1:] if bboxes is not None else None,\n",
    "            regions_p=regions_p,\n",
    "            regions_n=regions_n,\n",
    "            idx_p=idx_p_batch,\n",
    "            idx_n=idx_n_batch,\n",
    "            matched_bboxes=best_bbox_idx_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "model = MnistDetector(k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# ==================\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "i = 0\n",
    "start_index = i\n",
    "end_index = i + batch_size\n",
    "\n",
    "x_batch = x_train[start_index:end_index]  # TODO: maybe add light noise?\n",
    "y_batch = y_train[start_index:end_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_boxes = [utils.labels_to_tensor(yi, H, W) for yi in y_batch]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42, 128, 28, 28])\n",
      "torch.Size([42, 128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "detector_out = model(x_batch, y_boxes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([42, 128, 28, 28])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_out.regions_p[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}