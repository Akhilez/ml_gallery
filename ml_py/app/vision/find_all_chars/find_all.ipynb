{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from lib import detection_utils as utils\n",
    "from lib.mnist_aug.mnist_augmenter import DataManager, MNISTAug\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "k = 9\n",
    "H = 112\n",
    "W = 112\n",
    "Wp = 22\n",
    "Hp = 22\n",
    "b_regions = 256\n",
    "\n",
    "threshold_p = 0.6\n",
    "threshold_n = 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dm = DataManager()\n",
    "dm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "aug = MNISTAug()\n",
    "x_train, y_train = aug.get_augmented(dm.x_train, dm.y_train, 10)\n",
    "x_test, y_test = aug.get_augmented(dm.x_test, dm.y_test, 2)\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).view((-1, 1, H, W))\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).view((-1, 1, H, W))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MnistDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.threshold_p = 0.6\n",
    "        self.threshold_n = 0.3\n",
    "\n",
    "        self.Wp = 22\n",
    "        self.Hp = 22\n",
    "\n",
    "        self.X = 28  # Width of region\n",
    "        self.Y = 28\n",
    "\n",
    "        self.b_regions = 256\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        self.DetectorOut = collections.namedtuple('DetectorOut', 'features confidences diffs regions_p regions_n idx_p idx_n matched_bboxes')\n",
    "        self.anchors_tensor = utils.generate_anchors(shape=(Wp, Hp), sizes=(.15, .45, .75),\n",
    "                                        ratios=(0.5, 1, 2))  # Tensor of shape (4, k*H*W) -> cy, cy, w, h\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        self.box_regressor = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 5 * self.k, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y_bboxes=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        x: tensor of shape (-1, C, H, W)\n",
    "        bboxes: (optional) list of tensors of shape (4, n)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(x)\n",
    "        bboxes = self.box_regressor(features)\n",
    "        bboxes = bboxes.view((-1, 5, k, *bboxes.shape[-2:]))\n",
    "\n",
    "        regions_p = []\n",
    "        regions_n = []\n",
    "        idx_p_batch = []\n",
    "        idx_n_batch = []\n",
    "        best_bbox_idx_batch = []\n",
    "\n",
    "        # If training mode, then sample positives and negatives, extract regions\n",
    "        if self.training and y_bboxes is not None:\n",
    "            for i_batch in range(len(x)):\n",
    "                iou = utils.get_iou_map(y_bboxes[i], self.anchors_tensor)\n",
    "                iou = utils.raise_bbox_iou(iou, self.threshold_p)\n",
    "                iou_max, iou_argmax = torch.max(iou, 0)  # Shape (k*H*W)\n",
    "                idx_p, idx_n = utils.sample_pn_indices(iou_max, self.threshold_p, self.threshold_n, self.b_regions)\n",
    "                pred_bbox_p, pred_bbox_n = utils.get_pred_boxes(bboxes[i, 1:], self.anchors_tensor, idx_p, idx_n)  # (4, n) (cx, cy, w, h)\n",
    "                big_box_indices_p = utils.get_tiny_box_indices(pred_bbox_p, 0.05)\n",
    "                big_box_indices_n = utils.get_tiny_box_indices(pred_bbox_n, 0.05)\n",
    "                pred_bbox_p = pred_bbox_p[:, big_box_indices_p]\n",
    "                pred_bbox_n = pred_bbox_n[:, big_box_indices_n]\n",
    "                idx_p = idx_p[big_box_indices_p]\n",
    "                idx_n = idx_n[big_box_indices_n]\n",
    "                print(pred_bbox_p[:, 0])\n",
    "                pred_bbox_p = utils.centers_to_diag(pred_bbox_p)  # shape (4, p) (x1y1x2y2)\n",
    "                pred_bbox_n = utils.centers_to_diag(pred_bbox_n)\n",
    "                print(pred_bbox_p[:, 0])\n",
    "\n",
    "                idx_p_batch.append(idx_p)\n",
    "                idx_n_batch.append(idx_n)\n",
    "                best_bbox_idx_batch.append(iou_argmax)\n",
    "\n",
    "                # De-Normalize - Make coordinates feature indices b/w H and W\n",
    "                multiplier = torch.tensor([self.Wp, self.Hp, self.Wp, self.Hp]).view((4, 1))\n",
    "                pred_bbox_p = (pred_bbox_p * multiplier).type(torch.int32)  # shape (4, p) (x1y1x2y2)\n",
    "                pred_bbox_n = (pred_bbox_n * multiplier).type(torch.int32)\n",
    "                # Make crops of features\n",
    "                regions_batch = []\n",
    "                for positive_idx in range(len(idx_p)):\n",
    "                    idx = pred_bbox_p[positive_idx]\n",
    "                    cropped = features[i_batch, :, idx[0]:idx[2]+1, idx[1]:idx[3]+1]\n",
    "                    print(cropped.shape)\n",
    "                    print(idx)\n",
    "                    cropped = F.interpolate(cropped, (self.X, self.Y), mode='bilinear')\n",
    "                    regions_batch.append(cropped)\n",
    "                regions_p.append(torch.tensor(regions_batch))\n",
    "\n",
    "                regions_batch = []\n",
    "                for negative_idx in range(len(idx_n)):\n",
    "                    idx = pred_bbox_n[negative_idx]\n",
    "                    cropped = features[i_batch, :, idx[0]:idx[2]+1, idx[1]:idx[3]+1]\n",
    "                    cropped = F.interpolate(cropped, (self.X, self.Y), mode='bilinear')\n",
    "                    regions_batch.append(cropped)\n",
    "                regions_n.append(torch.tensor(regions_batch))\n",
    "\n",
    "        # TODO: If eval mode, then sample top 300 confidence anchors' regions\n",
    "        if not self.training:\n",
    "            pass\n",
    "\n",
    "        return self.DetectorOut(\n",
    "            features=features,\n",
    "            confidences=bboxes[:, 0] if bboxes is not None else None,\n",
    "            diffs=bboxes[:, 1:] if bboxes is not None else None,\n",
    "            regions_p=regions_p,\n",
    "            regions_n=regions_n,\n",
    "            idx_p=idx_p_batch,\n",
    "            idx_n=idx_n_batch,\n",
    "            matched_bboxes=best_bbox_idx_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = MnistDetector(k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# ==================\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "i = 0\n",
    "start_index = i\n",
    "end_index = i + batch_size\n",
    "\n",
    "x_batch = x_train[start_index:end_index]\n",
    "y_batch = y_train[start_index:end_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_boxes = [utils.labels_to_tensor(yi, H, W) for yi in y_batch]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3741, 0.6317, 0.4424, 0.4492], grad_fn=<SelectBackward>)\n",
      "tensor([0.1529, 0.4071, 0.5953, 0.8563], grad_fn=<SelectBackward>)\n",
      "torch.Size([128, 10, 1])\n",
      "tensor([ 3, 12, 12, 12, 11, 13,  0,  0, 14,  4,  4, 15,  6, 14, 15, 13,  2, 11,\n",
      "        14,  4, 13,  5,  3,  7,  4,  1,  1,  2,  3,  0, 12,  5,  1,  2, 11,  0,\n",
      "         4,  4,  3,  2, 15, 15], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhil/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "size shape must match input shape. Input is 1D, size is 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-9864a59aaab7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdetector_out\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_boxes\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-823c58789702>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x, y_bboxes)\u001B[0m\n\u001B[1;32m    126\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcropped\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m                     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m                     \u001B[0mcropped\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minterpolate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcropped\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mY\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'bilinear'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m                     \u001B[0mregions_batch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcropped\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m                 \u001B[0mregions_p\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mregions_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36minterpolate\u001B[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001B[0m\n\u001B[1;32m   3077\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3078\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3079\u001B[0;31m                 raise ValueError('size shape must match input shape. '\n\u001B[0m\u001B[1;32m   3080\u001B[0m                                  'Input is {}D, size is {}'.format(dim, len(size)))\n\u001B[1;32m   3081\u001B[0m             \u001B[0moutput_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: size shape must match input shape. Input is 1D, size is 2"
     ]
    }
   ],
   "source": [
    "detector_out = model(x_batch, y_boxes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}