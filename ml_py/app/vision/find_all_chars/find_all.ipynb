{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torchvision import ops\n",
    "\n",
    "from lib import detection_utils as utils\n",
    "from lib.mnist_aug.mnist_augmenter import DataManager, MNISTAug\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = 9\n",
    "H = 112\n",
    "W = 112\n",
    "Wp = 22\n",
    "Hp = 22\n",
    "b_regions = 256\n",
    "\n",
    "threshold_p = 0.6\n",
    "threshold_n = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dm = DataManager()\n",
    "dm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aug = MNISTAug()\n",
    "x_train, y_train = aug.get_augmented(dm.x_train, dm.y_train, 10)\n",
    "x_test, y_test = aug.get_augmented(dm.x_test, dm.y_test, 2)\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).view((-1, 1, H, W))\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).view((-1, 1, H, W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MnistDetector(nn.Module):\n",
    "\n",
    "    def __init__(self, k):\n",
    "        super().__init__()\n",
    "\n",
    "        self.threshold_p = 0.6\n",
    "        self.threshold_n = 0.3\n",
    "\n",
    "        self.Wp = 22\n",
    "        self.Hp = 22\n",
    "\n",
    "        self.X = 28  # Width of region\n",
    "        self.Y = 28\n",
    "\n",
    "        self.b_regions = 256\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        self.DetectorOut = collections.namedtuple('DetectorOut', ['features', 'confidences', 'diffs', 'regions_p', 'regions_n', 'pred_bbox_p', 'pred_bbox_n', 'idx_p', 'idx_n', 'matched_bboxes', 'iou_max'])\n",
    "        self.anchors_tensor = utils.generate_anchors(shape=(Wp, Hp), sizes=(.15, .45, .75),\n",
    "                                        ratios=(0.5, 1, 2))  # Tensor of shape (4, k*H*W) -> cy, cy, w, h\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        self.box_regressor = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 5 * self.k, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1152, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, y_bboxes=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ---------\n",
    "        x: tensor of shape (-1, C, H, W)\n",
    "        y_bboxes: (optional) list of tensors of shape (4, n)\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(x)\n",
    "        bboxes = self.box_regressor(features)\n",
    "        bboxes = bboxes.view((-1, 5, k, *bboxes.shape[-2:]))\n",
    "        confidences = F.sigmoid(bboxes[:, 0])\n",
    "\n",
    "        regions_p = []\n",
    "        regions_n = []\n",
    "        pred_bbox_p_batch = []\n",
    "        pred_bbox_n_batch = []\n",
    "        idx_p_batch = []\n",
    "        idx_n_batch = []\n",
    "        best_bbox_idx_batch = []\n",
    "        iou_max_batch = []\n",
    "\n",
    "        # If training mode, then sample positives and negatives, extract regions\n",
    "        if self.training and y_bboxes is not None:\n",
    "            for i_batch in range(len(x)):\n",
    "                iou = utils.get_iou_map(y_bboxes[i], self.anchors_tensor)\n",
    "                iou = utils.raise_bbox_iou(iou, self.threshold_p)\n",
    "                iou_max, iou_argmax = torch.max(iou, 0)  # Shape (k*H*W)\n",
    "\n",
    "                # Random sampling\n",
    "                idx_p, idx_n = utils.sample_pn_indices(iou_max, self.threshold_p, self.threshold_n, self.b_regions)\n",
    "\n",
    "                # Get off-set boxes\n",
    "                pred_bbox_p, pred_bbox_n = utils.get_pred_boxes(bboxes[i, 1:], self.anchors_tensor, idx_p, idx_n)  # (4, n) (cx, cy, w, h)\n",
    "\n",
    "                # Remove tiny boxes\n",
    "                big_box_indices_p = utils.get_tiny_box_indices(pred_bbox_p, 0.05)\n",
    "                big_box_indices_n = utils.get_tiny_box_indices(pred_bbox_n, 0.05)\n",
    "                pred_bbox_p = pred_bbox_p[:, big_box_indices_p]\n",
    "                pred_bbox_n = pred_bbox_n[:, big_box_indices_n]\n",
    "                idx_p = idx_p[big_box_indices_p]\n",
    "                idx_n = idx_n[big_box_indices_n]\n",
    "\n",
    "                # Change format from (cx cy w h) to (x1 y1 x2 y2)\n",
    "                pred_bbox_p = utils.centers_to_diag(pred_bbox_p)  # shape (4, p) (x1y1x2y2)\n",
    "                pred_bbox_n = utils.centers_to_diag(pred_bbox_n)\n",
    "\n",
    "                # Make record of these\n",
    "                idx_p_batch.append(idx_p)\n",
    "                idx_n_batch.append(idx_n)\n",
    "                best_bbox_idx_batch.append(iou_argmax)\n",
    "                iou_max_batch.append(iou_max)\n",
    "                pred_bbox_p_batch.append(pred_bbox_p)\n",
    "                pred_bbox_n_batch.append(pred_bbox_n)\n",
    "\n",
    "                # De-Normalize - Make coordinates feature indices b/w H and W\n",
    "                multiplier = torch.tensor([self.Wp, self.Hp, self.Wp, self.Hp]).view((4, 1))\n",
    "                pred_bbox_p = (pred_bbox_p * multiplier).round().type(torch.int32)  # shape (4, p) (x1y1x2y2)\n",
    "                pred_bbox_n = (pred_bbox_n * multiplier).round().type(torch.int32)\n",
    "\n",
    "                # Clip boxes that are out of range\n",
    "                pred_bbox_p = ops.clip_boxes_to_image(pred_bbox_p.T, (self.Hp, self.Wp)).T\n",
    "                pred_bbox_n = ops.clip_boxes_to_image(pred_bbox_n.T, (self.Hp, self.Wp)).T\n",
    "\n",
    "                # Make crops of features\n",
    "                regions_batch = []\n",
    "                for positive_idx in range(len(idx_p)):\n",
    "                    idx = pred_bbox_p[:, positive_idx]\n",
    "                    cropped = features[i_batch, :, idx[0]:idx[2]+1, idx[1]:idx[3]+1]\n",
    "                    cropped = F.interpolate(cropped.view((1, *cropped.shape)), (self.X, self.Y), mode='bilinear')[0]\n",
    "                    regions_batch.append(cropped)\n",
    "                regions_batch = torch.stack(regions_batch)\n",
    "                regions_p.append(regions_batch)\n",
    "\n",
    "                regions_batch = []\n",
    "                for negative_idx in range(len(idx_n)):\n",
    "                    idx = pred_bbox_n[:, negative_idx]\n",
    "                    cropped = features[i_batch, :, idx[0]:idx[2]+1, idx[1]:idx[3]+1]\n",
    "                    cropped = F.interpolate(cropped.view((1, *cropped.shape)), (self.X, self.Y), mode='bilinear')[0]\n",
    "                    regions_batch.append(cropped)\n",
    "                regions_n.append(torch.stack(regions_batch))\n",
    "\n",
    "        # TODO: If eval mode, then sample top 300 confidence anchors' regions\n",
    "        if not self.training:\n",
    "            pass\n",
    "\n",
    "        return self.DetectorOut(\n",
    "            features=features,\n",
    "            confidences=confidences,\n",
    "            diffs=bboxes[:, 1:],\n",
    "            regions_p=regions_p,\n",
    "            regions_n=regions_n,\n",
    "            pred_bbox_p=pred_bbox_p_batch,\n",
    "            pred_bbox_n=pred_bbox_n_batch,\n",
    "            idx_p=idx_p_batch,\n",
    "            idx_n=idx_n_batch,\n",
    "            matched_bboxes=best_bbox_idx_batch,\n",
    "            iou_max=iou_max_batch,\n",
    "        )\n",
    "\n",
    "model = MnistDetector(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhil/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/nn/functional.py:3060: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n"
     ]
    }
   ],
   "source": [
    "# ==================\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "i = 0\n",
    "start_index = i\n",
    "end_index = i + batch_size\n",
    "\n",
    "x_batch = x_train[start_index:end_index]  # TODO: maybe add light noise?\n",
    "y_batch = y_train[start_index:end_index]\n",
    "\n",
    "y_boxes = [utils.labels_to_tensor(yi, H, W) for yi in y_batch]\n",
    "\n",
    "detector_out = model(x_batch, y_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 22, 22])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape: (batch, k, H, W) | ones and zeros tensor.\n",
    "confidences_labels = utils.get_confidences(\n",
    "    torch.stack(detector_out.iou_max),\n",
    "    threshold_p,\n",
    "    (batch_size, model.k, model.Hp, model.Wp)\n",
    ")\n",
    "confidences_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-875c052dbfab>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m diffs_labels = torch.stack([\n\u001B[0m\u001B[1;32m      2\u001B[0m     utils.get_diffs(\n\u001B[1;32m      3\u001B[0m         \u001B[0my_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manchors_tensor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mdetector_out\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miou_max\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi_batch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-26-875c052dbfab>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m diffs_labels = torch.stack([\n\u001B[0;32m----> 2\u001B[0;31m     utils.get_diffs(\n\u001B[0m\u001B[1;32m      3\u001B[0m         \u001B[0my_batch\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0manchors_tensor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mdetector_out\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miou_max\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi_batch\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/ml_gallery/ml_py/lib/detection_utils.py\u001B[0m in \u001B[0;36mget_diffs\u001B[0;34m(bboxes, anchors, max_iou, argmax_iou, k, H, W)\u001B[0m\n\u001B[1;32m    173\u001B[0m     \u001B[0minvalid_indices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnonzero\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_iou\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 175\u001B[0;31m     \u001B[0mbboxes_max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbboxes\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margmax_iou\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    176\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    177\u001B[0m     \u001B[0mtx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mbboxes_max\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0manchors\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0manchors\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "diffs_labels = torch.stack([\n",
    "    utils.get_diffs(\n",
    "        y_boxes[i],\n",
    "        model.anchors_tensor,\n",
    "        detector_out.iou_max[i_batch],\n",
    "        detector_out.matched_bboxes[i_batch],\n",
    "        model.k,\n",
    "        model.Hp,\n",
    "        model.Wp\n",
    "    )  # Shape: (4, k, H, W)\n",
    "    for i_batch in range(batch_size)\n",
    "])\n",
    "diffs_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 2, 2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector_out.matched_bboxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confidences_loss_fn = nn.BCELoss()\n",
    "diffs_loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confidences_loss = confidences_loss_fn(detector_out.confidences, confidences_labels)\n",
    "confidences_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diffs_loss = diffs_loss_fn(detector_out.diffs, diffs_labels)\n",
    "diffs_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_loss = confidences_loss + diffs_loss\n",
    "\n",
    "optimizer.zero_grad()\n",
    "total_loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nms_boxes = []\n",
    "for i_batch in range(batch_size):\n",
    "    pred_boxes = torch.cat((detector_out.pred_bbox_n[i_batch].T, detector_out.pred_bbox_p[i_batch].T)).T\n",
    "\n",
    "    confidences_batch = detector_out.confidences[i_batch].flatten()\n",
    "    confidences_batch_p = confidences_batch[detector_out.idx_p[i_batch]]\n",
    "    confidences_batch_n = confidences_batch[detector_out.idx_n[i_batch]]\n",
    "    confidences_batch = torch.cat((confidences_batch_n, confidences_batch_p))\n",
    "\n",
    "    nms_indices = ops.nms(pred_boxes.T, confidences_batch, 0.7)\n",
    "    nms_boxes_i = pred_boxes[:, nms_indices]\n",
    "\n",
    "    print(nms_boxes_i.shape)\n",
    "    nms_boxes.append(utils.tensor_to_labels(nms_boxes_i, H, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i_batch in range(batch_size):\n",
    "    DataManager.plot_num(x_batch[i].view((H, W)), nms_boxes[i_batch])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}