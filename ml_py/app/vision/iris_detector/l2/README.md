This aim of this challenge to assess your skills in applying creative and quantitative computer vision and deep learning techniques to measuring the location and diameters of a subject’s pupils and iris from a cropped eye image.

Included here are 500 random images from our Training dataset:
https://www.dropbox.com/sh/wsxpcjcr0f4exg4/AAC8yyF0Q8qFHLmvw07QlAcUa?dl=0

The challenge is specifically as follows:
- Estimate the average diameter (in pixels) of both a) the pupil and b) the iris in each image.
- Alternatively, you can treat these objects as ellipses and estimate the major and minor axes.
- Train a deep learning model with these images and demonstrate the model’s performance on the testing images provided in the link.
- The same approach must be applied to each image.
- You may choose to implement any state-of-the-art model and may also use pre-trained weights to speed up the training process.
- We are interested in seeing how different your approach is from an existing off-the-shelf model, catering to our problem.
- We will go through the code to understand your implementation in depth and to get a sense of your coding style.
- The challenge will be assessed by maximizing accuracy of the inferred diameter across the provided testing images.
- Due: Within 7 days from this email.

Note: In the provided mask images, channel '0' refers to the background, channel '1' to the pupil and channel '2' refers to the iris.

An example of a cropped eye image and its’ corresponding segmentation mask is shown below.

(After this thoroughly reading and completing this question, please now skip down to the question which prompts you to submit your response as CODE by uploading file(s))


Q&A:
1. How is the training data generated by BioTrillion?
The data is recorded on mobile phones using the front/back cameras (orientation: portrait) at 30/60 fps at a resolution of 1080p or above. Using an annotation tool, an annotator clicks on multiple points on the iris boundary and the pupil boundary of different eye frames for a video. Using annotations recorded for an image frame, we fit the best ellipse to the points and use the center co-ordinates and the radius values of this fitted ellipse as the ground truth. The higher the number of points on the pupil/iris boundary, the better is the fit and the ground truth is more accurate.

2. How is the ground truth annotated? How would an annotator draw points over the pupil/iris during annotations?
When annotating an image, we asked the annotators to mark iris points where the intensity was midway between the sclera and the darkest part of the iris (from the outside-in), and the pupil points where the intensity was midway between the black pupil and the iris (from inside-out). This was the approach that we could helped the annotators be most consistent. To understand this better, I have included examples of QA images we generate internally during the annotation stage. If you zoom into the images 1 and 2 (at the end of the document), you will notice the annotated points (in blue color) on the pupil and the iris boundary.

3. The pupil and the iris can be circular/elliptical in shape due to the user’s gaze or head turn. How should I be handling such segmentation output to compare radius values?
As mentioned above, the annotation process involves a human clicking on several points along the boundary of the iris and sclera, and pupil and iris. The ellipse fit's axes are calculated from those points. Consequently, the ground truth values are continuous. The method by which your code estimates the major axis should use more information than just the distance between two opposing points; it should take advantage more of the information the segmentation provides. By doing so, your estimates will be a continuous variable, not quantized in half-pixel steps. You will use the major-axis value in the ground truth CSV file to compare it with the predicted radius value from your code.

4. Are they training images scaled based on the resolution they were captured?
Among the images we sent, all were from 1080p sources. Selfies acquired at this scale were not scaled but cropped to the right size. Images with higher resolution are downscaled and then cropped.

5. Different cameras used for video capturing will have different intrinsic parameters. Do you keep the same actual spatial resolution, or just pixelwise?
Indeed, they do. All analysis has been performed pixelwise without rectifying the image (which would be nice). It is an undue burden to have each user calibrate his or her phone prior to using the app. We have other ways of converting a pupil diameter into a useful metric that is not pixels.

6. What algorithms do you use for down/up-sampling? NN, linear, bicubic, more advanced?
Per the above, there has not been down/up-sampling. We use OpenCV’s bicubic interpolation method.

7. What are the pre-processing methods applied to the images before training?
The provided images have had no manipulation performed on them other than those done automatically on the phones. The only pre-processing done to the images is scaling (if necessary) and cropping.

8. How are the eyes cropped from the faces?
From the annotation, we have x_center and y_center of both the iris and pupil. We use the pupil’s center for the cropping. delta_x is half the desired width of the CNN input images and delta_y is half the desired height of the CNN input images.