{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from matplotlib.patches import Ellipse\n",
    "import os\n",
    "\n",
    "from settings import BASE_DIR\n",
    "\n",
    "height = 160\n",
    "width = 224\n",
    "\n",
    "\n",
    "def plot_img(image, ellipses=None, show=False):\n",
    "    \"\"\"\n",
    "    image: np.array of shape (c, h, w)\n",
    "    ellipses: np.array of shape (n, 5)\n",
    "    \"\"\"\n",
    "    plt.imshow(np.moveaxis(np.array(image), 0, -1))\n",
    "\n",
    "    _, h, w = image.shape\n",
    "\n",
    "    if ellipses is not None and len(ellipses) > 0:\n",
    "        for ellipse in ellipses:\n",
    "            xc, yc, rx, ry, a = ellipse\n",
    "            plt.gca().add_patch(\n",
    "                Ellipse(xy=(xc, yc), width=2 * rx, height=2 * ry, angle=a, fill=False)\n",
    "            )\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=height, width=width),\n",
    "        # A.RandomSizedCrop(min_max_height=(250, 250), height=300, width=400, p=0.5),\n",
    "        # A.CenterCrop(height=200, width=200),\n",
    "        # A.ToGray(p=0.2),\n",
    "        # A.ChannelDropout(channel_drop_range=(1, 2), p=0.2),\n",
    "        # A.ChannelShuffle(p=0.2),\n",
    "        # A.HueSaturationValue(p=0.2),\n",
    "        # A.ImageCompression(quality_lower=60, p=0.1),\n",
    "        # A.Posterize(p=0.2),\n",
    "        # A.Rotate(limit=40, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255),\n",
    "    ],\n",
    "    # keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
    ")\n",
    "\n",
    "\n",
    "class IrisImageDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path, labels_path=None, transform=None):\n",
    "        super(IrisImageDataset, self).__init__()\n",
    "        self.data = []\n",
    "        self.images_path = images_path\n",
    "        self.labels_path = labels_path\n",
    "        self.masks_path = masks_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_names = self.get_images_list(masks_path, file_ext=\".png\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(f\"{self.images_path}/{image_name}.png\")\n",
    "        image = np.array(image)\n",
    "\n",
    "        mask = Image.open(f\"{self.masks_path}/{image_name}.png\")\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        # Covert from channels last to channels first\n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "        mask = np.moveaxis(mask, -1, 0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    @staticmethod\n",
    "    def get_images_list(images_dir, file_ext=None):\n",
    "        files_list = sorted(os.listdir(images_dir))\n",
    "        extension_len = len(file_ext)\n",
    "        if file_ext:\n",
    "            file_list_ = []\n",
    "            for file_name in files_list:\n",
    "                if file_name[-extension_len:] == file_ext:\n",
    "                    file_list_.append(file_name[:-extension_len])\n",
    "            files_list = file_list_\n",
    "        return files_list\n",
    "\n",
    "\n",
    "data_dir = f\"{BASE_DIR}/data/pupil/L2\"\n",
    "train_images_path = f\"{data_dir}/training_set/images\"\n",
    "training_labels_path = f\"{data_dir}/training_set/ground_truth\"\n",
    "training_masks_path = f\"{data_dir}/training_set/masks\"\n",
    "\n",
    "dataset = IrisImageDataset(\n",
    "    images_path=train_images_path, masks_path=training_masks_path, transform=transform\n",
    ")\n",
    "train_loader = DataLoader(dataset, batch_size=3, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class IrisUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisUNet, self).__init__()\n",
    "\n",
    "        self.modules = [\n",
    "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(8, 3, kernel_size=3, stride=1, padding=1),\n",
    "        ]\n",
    "        self.n = len(self.modules)\n",
    "        self.modules_list = nn.ModuleList(self.modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        compressions = []\n",
    "\n",
    "        for i in range(self.n // 2):\n",
    "            x = self.modules[i](x)\n",
    "            x = torch.relu(x)\n",
    "            compressions.append(x)\n",
    "\n",
    "        x = torch.tensor(0)\n",
    "\n",
    "        for i in range(self.n//2, self.n - 1):\n",
    "            x = x + compressions.pop()\n",
    "            x = self.modules[i](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        x = x + compressions[-1]\n",
    "        x = self.modules[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = IrisUNet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (56) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-35-5c70340a0065>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmasks\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    726\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 727\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-34-605b50c036d8>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mcompressions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (28) must match the size of tensor b (56) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    y = model(images)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "F.cross\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}