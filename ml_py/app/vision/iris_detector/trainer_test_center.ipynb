{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akhil/code/ml_gallery/ml_py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from settings import BASE_DIR"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim):\n",
    "        super(SelfAttention,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "\n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) #\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature\n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "\n",
    "        out = self.gamma*out + x\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "def plot_img(image, landmarks=None, circles=None, circles2=None, landmarks2=None):\n",
    "    \"\"\"\n",
    "    image: np.array of shape (c, h, w)\n",
    "    landmarks: np.array of shape (n, 2)\n",
    "    circles: np.array of shape (n, 3)\n",
    "    \"\"\"\n",
    "    plt.imshow(np.moveaxis(np.array(image), 0, -1))\n",
    "\n",
    "    _, h, w = image.shape\n",
    "\n",
    "    if landmarks is not None and len(landmarks) > 0:\n",
    "        x = landmarks[:, 0]\n",
    "        y = landmarks[:, 1]\n",
    "        plt.scatter(x, y)\n",
    "\n",
    "    if circles is not None and len(circles) > 0:\n",
    "        for circle in circles:\n",
    "            xc, yc, r = circle\n",
    "            plt.gca().add_patch(plt.Circle((xc, yc), r, fill=False))\n",
    "\n",
    "    if landmarks2 is not None and len(landmarks2) > 0:\n",
    "        x = landmarks2[:, 0]\n",
    "        y = landmarks2[:, 1]\n",
    "        plt.scatter(x, y, color='red')\n",
    "\n",
    "    if circles2 is not None and len(circles2) > 0:\n",
    "        for circle in circles2:\n",
    "            xc, yc, r = circle\n",
    "            plt.gca().add_patch(plt.Circle((xc, yc), r, color='red', fill=False))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "transform = A.Compose (\n",
    "    [\n",
    "        A.Resize(height=300, width=400),\n",
    "        # A.RandomSizedCrop(min_max_height=(250, 250), height=300, width=400, p=0.5),\n",
    "        # A.CenterCrop(height=200, width=200),\n",
    "        # A.ToGray(p=0.2),\n",
    "        # A.ChannelDropout(channel_drop_range=(1, 2), p=0.2),\n",
    "        # A.ChannelShuffle(p=0.2),\n",
    "        # A.HueSaturationValue(p=0.2),\n",
    "        # A.ImageCompression(quality_lower=60, p=0.1),\n",
    "        # A.Posterize(p=0.2),\n",
    "        # A.Rotate(limit=40, p=0.5, border_mode=cv2.BORDER_CONSTANT),\n",
    "        # A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0,0,0], std=[1,1,1], max_pixel_value=255),\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def normalize_inner_width(inner_widths: np.array):\n",
    "    return inner_widths / (400 * 0.25)\n",
    "\n",
    "def normalize_outer_width(widths: np.array):\n",
    "    return widths / (400 * 0.50)\n",
    "\n",
    "def denormalize_inner_width(inner_widths: np.array):\n",
    "    return inner_widths * (400 * 0.25)\n",
    "\n",
    "def denormalize_outer_width(widths: np.array):\n",
    "    return widths * (400 * 0.50)\n",
    "\n",
    "def normalize_center(center: np.array):\n",
    "    # Find offset\n",
    "    offset = center - np.array([300 / 2, 400 / 2])\n",
    "    return offset / [300, 400]\n",
    "\n",
    "def denormalize_center(center: np.array):\n",
    "    center_ = torch.tensor([300 / 2, 400 / 2])\n",
    "    return center_ + center * torch.tensor([300, 400])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class IrisImageDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_path, transform=None):\n",
    "        super(IrisImageDataset, self).__init__()\n",
    "        self.data = []\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_path = labels_path\n",
    "        self.transform = transform\n",
    "        self.height = 300\n",
    "        self.width = 400\n",
    "\n",
    "        with open(labels_path) as json_file:\n",
    "            self.labels = json.load(json_file)\n",
    "\n",
    "        self.image_names = sorted(list(self.labels.keys()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.image_names[index]\n",
    "        label = self.labels[image_name]\n",
    "        image = Image.open(f'{self.images_dir}/{image_name}.tiff')\n",
    "        image = np.array(image)\n",
    "\n",
    "        inner_landmarks = label['inner']['landmarks']\n",
    "        outer_landmarks = label['outer']['landmarks']\n",
    "        inner_circle = label['inner'].get('circles')\n",
    "        center = [[inner_circle['xc'], inner_circle['yc']]] if inner_circle else []\n",
    "\n",
    "        landmarks = inner_landmarks + outer_landmarks + center\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, keypoints=landmarks)\n",
    "            image = augmentations['image']\n",
    "            landmarks = augmentations['keypoints']\n",
    "\n",
    "        n_inner = len(inner_landmarks)\n",
    "        n_outer = len(outer_landmarks)\n",
    "\n",
    "        inner = np.array(landmarks[:n_inner])\n",
    "        outer = np.array(landmarks[n_inner: n_inner + n_outer])\n",
    "\n",
    "        labels = {\n",
    "            'inner': self.normalize_landmarks(inner).tolist(),\n",
    "            'outer': self.normalize_landmarks(outer).tolist(),\n",
    "            'center': np.array(landmarks[-1]),\n",
    "            'inner_width': normalize_inner_width(self.get_width(inner)),\n",
    "            'outer_width': normalize_outer_width(self.get_width(outer)),\n",
    "            'name': image_name\n",
    "        }\n",
    "\n",
    "        # Covert from channels last to channels first\n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def get_width( landmarks: np.array) -> float:\n",
    "            xs = landmarks[:, 0]\n",
    "            width = float(np.max(xs) - np.min(xs))\n",
    "            return width\n",
    "\n",
    "    def normalize_landmarks(self, landmarks):\n",
    "        return landmarks / [self.width, self.height]\n",
    "\n",
    "class IrisWidthsDataset(IrisImageDataset):\n",
    "    def __getitem__(self, index):\n",
    "        image, labels = super().__getitem__(index)\n",
    "        return image, (labels['inner_width'], labels['outer_width'], labels['center'][0], labels['center'][1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data_dir = f'{BASE_DIR}/data/pupil'\n",
    "images_dir = f'{data_dir}/train/images'\n",
    "labels_path = f'{data_dir}/train/labels.json'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "dataset = IrisWidthsDataset(images_dir=images_dir, labels_path=labels_path, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=3, shuffle=True)  #, collate_fn=lambda x: x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class IrisDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisDetector, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8, padding=1, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(8, 16, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, padding=1, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, padding=1, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # 38, 50\n",
    "\n",
    "            nn.Conv2d(32, 64, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, padding=1, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # 19, 25\n",
    "\n",
    "            SelfAttention(64),\n",
    "\n",
    "            # nn.Conv2d(128, 256, padding=1, kernel_size=3),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv2d(256, 256, padding=1, kernel_size=3, stride=2),\n",
    "            # nn.ReLU(),\n",
    "            # # 5, 7\n",
    "            #\n",
    "            # nn.Conv2d(256, 512, padding=1, kernel_size=3),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv2d(512, 64, padding=1, kernel_size=3, stride=2),\n",
    "            # nn.ReLU(),\n",
    "            # # 3, 4\n",
    "\n",
    "        )\n",
    "\n",
    "        self.center = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, padding=1, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # 10, 13\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(128 * 10 * 13, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.widths = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, padding=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, padding=1, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # 10, 13\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(128 * 10 * 13, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        center = self.center(features)\n",
    "\n",
    "\n",
    "\n",
    "        widths = self.widths(features)\n",
    "        return torch.cat((widths, center), 1)\n",
    "\n",
    "model = IrisDetector()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class IrisCenterDetector(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "\n",
    "        self.first = nn.Sequential(\n",
    "            nn.Conv2d(3, units[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(units[i], units[i + 1], kernel_size=3, padding=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                for i in range(len(units) - 1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv2d(units[-1], 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first(x)\n",
    "        for module in self.hidden:\n",
    "            x = module(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "model = IrisCenterDetector([8, 16, 32, 64, 32, 16, 8, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=1e-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68146)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(74953)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(80142)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(83748)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(59312)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(81345)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(69335)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(64142)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(76546)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(69744)\n",
      "tensor(0)\n",
      "11.695246696472168\n",
      "tensor(76913)\n",
      "tensor(0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-f7de4b18ab34>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m         \u001B[0moptim\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    219\u001B[0m                 \u001B[0mretain_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    220\u001B[0m                 create_graph=create_graph)\n\u001B[0;32m--> 221\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    222\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    223\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/ml_gallery/ml_py/venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m    128\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 130\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    131\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        labels = torch.stack(labels).T.float()\n",
    "        cx = labels[:, 2].type(torch.int)\n",
    "        cy = labels[:, 3].type(torch.int)\n",
    "\n",
    "        yh = model(images.float())\n",
    "        yh_max = torch.softmax(yh, 1)\n",
    "\n",
    "        yh_max = yh_max.flatten(1)\n",
    "\n",
    "        idx = torch.tensor(np.ravel_multi_index([cx, cy], (300, 400)))\n",
    "        print(idx[0])\n",
    "        print(torch.argmax(yh_max))\n",
    "\n",
    "        loss = F.cross_entropy(yh_max, idx)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        print(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.ravel_multi_index([[1,1], [2,2]], (3,3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}