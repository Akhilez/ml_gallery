{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def to_one_hot(x):\n",
    "    b = np.zeros((x.size, x.max()+1))\n",
    "    b[np.arange(x.size), x] = 1\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_path = \"../../data/mnist/\"\n",
    "x_test = torch.Tensor(load(f'{data_path}/x_test.npy'))\n",
    "x_train = torch.Tensor(load(f'{data_path}/x_train.npy'))\n",
    "y_test = torch.Tensor(to_one_hot(load(f'{data_path}/y_test.npy').astype(int)))\n",
    "y_train = torch.Tensor(to_one_hot(load(f'{data_path}/y_train.npy').astype(int)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.Size([4000, 784])\n",
      "torch.Size([4000, 10])\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(torch.min(x_test))\n",
    "print(torch.max(x_test))\n",
    "print(y_train[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_random(shape, min_=-0.5, max_=0.5):\n",
    "    return torch.FloatTensor(*shape).uniform_(min_, max_).requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "```yaml\n",
    "Generator:\n",
    "  - Input: 100 (90 noise, 10 one-hot)\n",
    "  - h1: 150\n",
    "  - h2: 400\n",
    "  - h3: 784\n",
    "\n",
    "FeatureExtractor:\n",
    "  - Input: 784 (28x28)\n",
    "  - h1: 400\n",
    "  - h2: 150\n",
    "  - h3: 100\n",
    "\n",
    "Discriminator:\n",
    "  - Input: 100\n",
    "  - h1: 50\n",
    "  - h2: 1\n",
    "\n",
    "Classifier:\n",
    "  - Input: 100\n",
    "  - h1: 50\n",
    "  - h2: 10\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((100, 150)), torch.zeros(150, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((150, 400)), torch.zeros(400, requires_grad=True)\n",
    "        self.w3, self.b3 = get_random((400, 784)), torch.zeros(784, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        # x.shape = (batch, 100)\n",
    "        h1 = x.matmul(self.w1) + self.b1\n",
    "        h2 = h1.matmul(self.w2) + self.b2\n",
    "        h3 = torch.nn.functional.softmax(h2.matmul(self.w3) + self.b3)\n",
    "        return h3\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        self.w3 = (self.w3 - lr * self.w3.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()\n",
    "        self.b3 = (self.b3 - lr * self.b3.grad).detach().requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((784, 400)), torch.zeros(400, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((400, 150)), torch.zeros(150, requires_grad=True)\n",
    "        self.w3, self.b3 = get_random((150, 100)), torch.zeros(100, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        h1 = torch.sigmoid(x.matmul(self.w1) + self.b1)\n",
    "        h2 = torch.sigmoid(h1.matmul(self.w2) + self.b2)\n",
    "        h3 = torch.sigmoid(h2.matmul(self.w3) + self.b3)\n",
    "        return h3\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        \n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        self.w3 = (self.w3 - lr * self.w3.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()\n",
    "        self.b3 = (self.b3 - lr * self.b3.grad).detach().requires_grad_()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((100, 50)), torch.zeros(50, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((50, 1)), torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        h1 = x.matmul(self.w1) + self.b1\n",
    "        h2 = torch.sigmoid(h1.matmul(self.w2) + self.b2)\n",
    "        return h2\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((100, 50)), torch.zeros(50, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((50, 10)), torch.zeros(10, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        h1 = torch.sigmoid(x.matmul(self.w1) + self.b1)\n",
    "        h2 = torch.softmax(h1.matmul(self.w2) + self.b2)\n",
    "        return h2\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "fe = FeatureExtractor()\n",
    "classifier = Classifier()\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "  Losses: [0.6287707017072819, 0.3818388574945857, 265.1557381749153]\n",
      "Epoch: 1\n",
      "  Losses: [0.10480095307352144, 0.11042106978129596, 254.13847422599792]\n",
      "Epoch: 2\n",
      "  Losses: [0.05033660190800404, 0.07669430607711547, 248.04603338241577]\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])) is deprecated. Please ensure they have the same size.\n",
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n",
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n",
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])) is deprecated. Please ensure they have the same size.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    losses = [0, 0, 0]\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        start_index = i\n",
    "        end_index = i+batch_size\n",
    "        \n",
    "        x_batch = x_train[start_index:end_index]\n",
    "        y_batch = y_train[start_index:end_index]\n",
    "    \n",
    "        \n",
    "        # ------------ Train with real image ----------------\n",
    "        real = torch.ones(batch_size)\n",
    "        fake = torch.zeros(batch_size)\n",
    "        \n",
    "        features = fe(x_batch)\n",
    "        discriminator_out = discriminator(features)\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy(discriminator_out, real)\n",
    "        \n",
    "        loss.backward()\n",
    "        discriminator.optimize(0.1)\n",
    "        fe.optimize(0.1)\n",
    "        \n",
    "        losses[0] += float(loss)\n",
    "        \n",
    "        # --------------- Train with fake image -------------------\n",
    "        \n",
    "        generator_input = np.random.uniform(0, 1, (batch_size, 90))\n",
    "        generator_input = np.concatenate((generator_input, y_batch), axis=1)\n",
    "        generator_input = torch.Tensor(generator_input)\n",
    "        \n",
    "        generated = generator(generator_input)\n",
    "        features = fe(generated)\n",
    "        discriminator_out = discriminator(features)\n",
    "        classifier_out = classifier(features)\n",
    "        \n",
    "        loss_discriminator = torch.nn.functional.binary_cross_entropy(discriminator_out, fake)\n",
    "        loss_classifier = torch.nn.functional.binary_cross_entropy(classifier_out, y_batch)\n",
    "        \n",
    "        loss = loss_discriminator + loss_classifier\n",
    "        loss.backward()\n",
    "        \n",
    "        discriminator.optimize(0.1)\n",
    "        classifier.optimize(0.1)\n",
    "        fe.optimize(0.1)\n",
    "        generator.optimize(0.1)\n",
    "        \n",
    "        losses[1] += float(loss_discriminator)\n",
    "        losses[2] += float(loss_classifier)\n",
    "        \n",
    "    print(f'  Losses: {losses}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      " Loss: 62.56604519486427\n",
      "Epoch: 1\n",
      " Loss: 53.11286461353302\n",
      "Epoch: 2\n",
      " Loss: 40.12722598016262\n",
      "Epoch: 3\n",
      " Loss: 30.553485609591007\n",
      "Epoch: 4\n",
      " Loss: 24.46516615524888\n",
      "Epoch: 5\n",
      " Loss: 20.358985599130392\n",
      "Epoch: 6\n",
      " Loss: 17.420197769999504\n",
      "Epoch: 7\n",
      " Loss: 15.226270627230406\n",
      "Epoch: 8\n",
      " Loss: 13.521448962390423\n",
      "Epoch: 9\n",
      " Loss: 12.143279342912138\n",
      "tensor([[3.6764e-03, 1.4693e-02, 5.2649e-01, 1.4998e-02, 2.1276e-04, 6.2665e-03,\n",
      "         3.3053e-04, 3.5649e-02, 3.9260e-01, 5.0789e-03],\n",
      "        [1.3938e-05, 9.8956e-01, 1.7119e-03, 1.9275e-03, 1.8419e-04, 1.5237e-03,\n",
      "         6.7148e-04, 1.0899e-03, 2.9270e-03, 3.8668e-04]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is just the classifier.\n",
    "\n",
    "batch_size = 20\n",
    "lr = 0.5\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    losses = 0\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        start_index = i\n",
    "        end_index = i+batch_size\n",
    "        \n",
    "        x_batch = x_train[start_index:end_index]\n",
    "        y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        features = fe(x_batch)\n",
    "        classes = classifier(features)\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy(classes, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        fe.optimize(lr)\n",
    "        classifier.optimize(lr)\n",
    "        \n",
    "        losses += loss.detach().numpy()\n",
    "    print(f' Loss: {losses}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    index = 133\n",
    "    x_sample = x_test[index:index+2]\n",
    "    y_sample = y_test[index:index+2]\n",
    "    y_pred = classifier(fe(x_sample))\n",
    "    print(y_pred)\n",
    "    print(y_sample)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}