{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def to_one_hot(x):\n",
    "    b = np.zeros((x.size, x.max()+1))\n",
    "    b[np.arange(x.size), x] = 1\n",
    "    return b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_path = \"../../data/mnist/\"\n",
    "x_test = torch.Tensor(load(f'{data_path}/x_test.npy'))\n",
    "x_train = torch.Tensor(load(f'{data_path}/x_train.npy'))\n",
    "y_test = torch.Tensor(to_one_hot(load(f'{data_path}/y_test.npy').astype(int)))\n",
    "y_train = torch.Tensor(to_one_hot(load(f'{data_path}/y_train.npy').astype(int)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.Size([4000, 784])\n",
      "torch.Size([4000, 10])\n",
      "tensor(0.)\n",
      "tensor(1.)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(torch.min(x_test))\n",
    "print(torch.max(x_test))\n",
    "print(y_train[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_random(shape, min_=-0.5, max_=0.5):\n",
    "    return torch.FloatTensor(*shape).uniform_(min_, max_).requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "```yaml\n",
    "Generator:\n",
    "  - Input: 100 (90 noise, 10 one-hot)\n",
    "  - h1: 150\n",
    "  - h2: 400\n",
    "  - h3: 784\n",
    "\n",
    "FeatureExtractor:\n",
    "  - Input: 784 (28x28)\n",
    "  - h1: 400\n",
    "  - h2: 150\n",
    "  - h3: 100\n",
    "\n",
    "Discriminator:\n",
    "  - Input: 100\n",
    "  - h1: 50\n",
    "  - h2: 1\n",
    "\n",
    "Classifier:\n",
    "  - Input: 100\n",
    "  - h1: 50\n",
    "  - h2: 10\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((100, 150)), torch.zeros(150, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((150, 400)), torch.zeros(400, requires_grad=True)\n",
    "        self.w3, self.b3 = get_random((400, 784)), torch.zeros(784, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        # x.shape = (batch, 100)\n",
    "        h1 = x.matmul(self.w1) + self.b1\n",
    "        h2 = h1.matmul(self.w2) + self.b2\n",
    "        h3 = torch.nn.functional.softmax(h2.matmul(self.w3) + self.b3)\n",
    "        return h3\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        self.w3 = (self.w3 - lr * self.w3.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()\n",
    "        self.b3 = (self.b3 - lr * self.b3.grad).detach().requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((784, 400)), torch.zeros(400, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((400, 150)), torch.zeros(150, requires_grad=True)\n",
    "        self.w3, self.b3 = get_random((150, 100)), torch.zeros(100, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        h1 = torch.nn.functional.sigmoid(x.matmul(self.w1) + self.b1)\n",
    "        h2 = torch.nn.functional.sigmoid(h1.matmul(self.w2) + self.b2)\n",
    "        h3 = torch.nn.functional.sigmoid(h2.matmul(self.w3) + self.b3)\n",
    "        return h3\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        \n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        self.w3 = (self.w3 - lr * self.w3.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()\n",
    "        self.b3 = (self.b3 - lr * self.b3.grad).detach().requires_grad_()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((100, 50)), torch.zeros(50, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((50, 1)), torch.zeros(1, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        h1 = x.matmul(self.w1) + self.b1\n",
    "        h2 = torch.nn.functional.tanh(h1.matmul(self.w2) + self.b2)\n",
    "        return h2\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "    \n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1, self.b1 = get_random((100, 50)), torch.zeros(50, requires_grad=True)\n",
    "        self.w2, self.b2 = get_random((50, 10)), torch.zeros(10, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        h1 = torch.nn.functional.sigmoid(x.matmul(self.w1) + self.b1)\n",
    "        h2 = torch.nn.functional.softmax(h1.matmul(self.w2) + self.b2)\n",
    "        return h2\n",
    "    \n",
    "    def optimize(self, lr):\n",
    "        self.w1 = (self.w1 - lr * self.w1.grad).detach().requires_grad_()\n",
    "        self.w2 = (self.w2 - lr * self.w2.grad).detach().requires_grad_()\n",
    "        \n",
    "        self.b1 = (self.b1 - lr * self.b1.grad).detach().requires_grad_()\n",
    "        self.b2 = (self.b2 - lr * self.b2.grad).detach().requires_grad_()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "fe = FeatureExtractor()\n",
    "classifier = Classifier()\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "lr = 0.5\n",
    "epochs = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      " Loss: 0.1742004120023921\n",
      "Epoch: 1\n",
      " Loss: 0.15941612530878047\n",
      "Epoch: 2\n",
      " Loss: 0.14681795617070748\n",
      "Epoch: 3\n",
      " Loss: 0.13594687085424084\n",
      "Epoch: 4\n",
      " Loss: 0.12647330131585477\n",
      "Epoch: 5\n",
      " Loss: 0.11815060551452916\n",
      "Epoch: 6\n",
      " Loss: 0.11078955536504509\n",
      "Epoch: 7\n",
      " Loss: 0.1042407589557115\n",
      "Epoch: 8\n",
      " Loss: 0.09838390069489833\n",
      "Epoch: 9\n",
      " Loss: 0.09312021703226492\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "d:\\repos\\ml_gallery\\ml_py\\venv\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    losses = 0\n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "        start_index = i\n",
    "        end_index = i+batch_size\n",
    "        \n",
    "        x_batch = x_train[start_index:end_index]\n",
    "        y_batch = y_train[start_index:end_index]\n",
    "        \n",
    "        features = fe(x_batch)\n",
    "        classes = classifier(features)\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy(classes, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        fe.optimize(lr)\n",
    "        classifier.optimize(lr)\n",
    "        \n",
    "        losses += loss.detach().numpy()\n",
    "    print(f' Loss: {losses}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}